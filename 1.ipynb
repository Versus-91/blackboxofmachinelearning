{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.27.0.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.27.0.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve, train_test_split, KFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly import tools\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import cufflinks as cf\n",
    "cf.go_offline()\n",
    "cf.set_config_file(offline=False, world_readable=True)\n",
    "import pathlib\n",
    "sns.set(style='white', context='notebook', palette='deep')\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "from sklearn.base import clone\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workclass: 9 unique values\n",
      "marital.status: 7 unique values\n",
      "occupation: 15 unique values\n",
      "race: 5 unique values\n",
      "sex: 2 unique values\n"
     ]
    }
   ],
   "source": [
    "data =  pd.read_csv(\"./datasets/adult.csv\")\n",
    "target_col = 'income'\n",
    "data[target_col] = np.where(data[target_col] == '<=50K', 0, 1)\n",
    "use_cols = list(set(data.columns) - set([target_col]))\n",
    "numerical = [\"age\", \"hours.per.week\", \"capital.gain\", \"capital.loss\"]\n",
    "categorical = data[use_cols].columns.difference(numerical).tolist()\n",
    "categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import lime\n",
    "import pdpbox\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "X = dataset.drop(columns='income')\n",
    "X = pd.get_dummies(X)\n",
    "Y = dataset['income']\n",
    "validation_size = 0.20\n",
    "seed = 7\n",
    "scoring = 'accuracy'\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y,\n",
    "    test_size=validation_size,random_state=seed)\n",
    "\n",
    "\n",
    "# Instantiate and fit the Logistic Regression model\n",
    "logreg = LogisticRegression().fit(X_train, y_train)\n",
    "\n",
    "# Predicting values\n",
    "y_pred_logreg = logreg.predict(X_test)\n",
    "\n",
    "accuracy_lr = accuracy_score(y_test, y_pred_logreg)\n",
    "f1_lr = f1_score(y_test, y_pred_logreg,average=\"micro\")\n",
    "precision_lr = precision_score(y_test, y_pred_logreg)\n",
    "recall_lr = recall_score(y_test, y_pred_logreg)\n",
    "roc_lr = roc_auc_score(y_test, y_pred_logreg)\n",
    "\n",
    "# Printing the test accuracy for Logistic Regression\n",
    "print(\"The test accuracy score of Logistic Regression is \", accuracy_score(\n",
    "    y_test, y_pred_logreg), \"f1 :\", f1_lr)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and fit the Decision Tree model\n",
    "dt = DecisionTreeClassifier(random_state=42).fit(X_train, y_train)\n",
    "\n",
    "# Predicting values\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "\n",
    "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
    "f1_dt = f1_score(y_test, y_pred_dt)\n",
    "precision_dt = precision_score(y_test, y_pred_dt)\n",
    "recall_dt = recall_score(y_test, y_pred_dt)\n",
    "roc_dt = roc_auc_score(y_test, y_pred_dt)\n",
    "\n",
    "# Printing the test accuracy for Decision Tree\n",
    "print(\"The test accuracy score of Decision Tree is \", accuracy_dt, \"f1 :\", f1_dt)\n",
    "\n",
    "# Instantiate and fit the SVM model\n",
    "svm = SVC(random_state=42, probability=True).fit(X_train, y_train)\n",
    "\n",
    "# Predicting values\n",
    "y_pred_svm = svm.predict(X_test)\n",
    "\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "f1_svm = f1_score(y_test, y_pred_svm)\n",
    "precision_svm = precision_score(y_test, y_pred_svm)\n",
    "recall_svm = recall_score(y_test, y_pred_svm)\n",
    "roc_svm = roc_auc_score(y_test, y_pred_svm)\n",
    "\n",
    "# Printing the test accuracy for SVM\n",
    "print(\"The test accuracy score of SVM is \", accuracy_score(\n",
    "    y_test, y_pred_svm), \"f1 :\",  f1_score(y_test, y_pred_svm))\n",
    "\n",
    "# Instantiate and fit the Random Forest model\n",
    "rf = RandomForestClassifier().fit(X_train, y_train)\n",
    "\n",
    "# Predicting values\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "f1_rf = f1_score(y_test, y_pred_rf)\n",
    "precision_rf = precision_score(y_test, y_pred_rf)\n",
    "recall_rf = recall_score(y_test, y_pred_rf)\n",
    "roc_rf = roc_auc_score(y_test, y_pred_rf)\n",
    "\n",
    "# Printing the test accuracy for Random Forest\n",
    "print(\"The test accuracy score of Random Forest is \", accuracy_score(\n",
    "    y_test, y_pred_rf), \"f1 :\",  f1_score(y_test, y_pred_rf))\n",
    "\n",
    "# Instantiate the XGBoost classifier\n",
    "xgb_classifier = xgb.XGBClassifier(n_estimators=50, random_state=42)\n",
    "\n",
    "# Fit the XGBoost model\n",
    "xgb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict values\n",
    "y_pred_xgb = xgb_classifier.predict(X_test)\n",
    "\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "f1_xgb = f1_score(y_test, y_pred_xgb)\n",
    "precision_xgb = precision_score(y_test, y_pred_xgb)\n",
    "recall_xgb = recall_score(y_test, y_pred_xgb)\n",
    "roc_xgb = roc_auc_score(y_test, y_pred_xgb)\n",
    "# Printing the test accuracy for Gradient Boosting Classifier\n",
    "print(\"The test accuracy score of Gradient Boosting Classifier is \",\n",
    "      accuracy_score(y_test, y_pred_xgb), \"f1 :\",  f1_score(y_test, y_pred_xgb))\n",
    "\n",
    "# Instantiate the mlp\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(64, 64),\n",
    "                    max_iter=2000, random_state=42)\n",
    "\n",
    "# Fit the mlp model\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# Predict values\n",
    "y_pred_mlp = mlp.predict(X_test)\n",
    "accuracy_rn = accuracy_score(y_test, y_pred_mlp)\n",
    "recall_rn = recall_score(y_test, y_pred_mlp)\n",
    "precision_rn = precision_score(y_test, y_pred_mlp)\n",
    "f1_rn = f1_score(y_test, y_pred_mlp)\n",
    "roc_rn = roc_auc_score(y_test, y_pred_mlp)\n",
    "# Printing the test accuracy for mlp Classifier\n",
    "print(\"The test accuracy score of MLP Classifier is \", accuracy_score(\n",
    "    y_test, y_pred_mlp), \"f1 :\",  f1_score(y_test, y_pred_mlp))\n",
    "\n",
    "# Instantiate the KNN\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "y_pred_knn = knn.predict(X_test)\n",
    "\n",
    "accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
    "f1_knn = f1_score(y_test, y_pred_knn)\n",
    "precision_knn = precision_score(y_test, y_pred_knn)\n",
    "recall_knn = recall_score(y_test, y_pred_knn)\n",
    "roc_knn = roc_auc_score(y_test, y_pred_knn)\n",
    "\n",
    "# Printing the test accuracy for mlp Classifier\n",
    "print(\"The test accuracy score of knn Classifier is \", accuracy_score(\n",
    "    y_test, y_pred_knn), \"f1 :\", f1_score(y_test, y_pred_knn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate permutation importance for SVM\n",
    "models = [\n",
    "    ('Decision Tree', accuracy_dt, recall_dt, precision_dt, f1_dt, roc_dt),\n",
    "    ('Random Forest', accuracy_rf, recall_rf, precision_rf, f1_rf, roc_rf),\n",
    "    ('XGBoost', accuracy_xgb, recall_xgb, precision_xgb, f1_xgb, roc_xgb),\n",
    "    ('kNN', accuracy_knn, recall_knn, precision_knn, f1_knn, roc_knn),\n",
    "    ('Logistic Regression', accuracy_lr, recall_lr, precision_lr, f1_lr, roc_lr),\n",
    "    ('SVM', accuracy_svm, recall_svm, precision_svm, f1_svm, roc_svm),\n",
    "    ('Neural Networks', accuracy_rn, recall_rn, precision_rn, f1_rn, roc_rn)]\n",
    "\n",
    "df_all_models = pd.DataFrame(models, columns=[\n",
    "                             'Model', 'Accuracy (%)', 'Recall (%)', 'Precision (%)', 'F1 (%)', 'AUC'])\n",
    "display(df_all_models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
