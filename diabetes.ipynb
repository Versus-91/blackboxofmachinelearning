{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import pandas as pd\n",
    "from pandas import set_option\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "df = pd.read_csv(\"./datasets/diabetes.csv\")\n",
    "df_name=df.columns\n",
    "\n",
    "X =  df[df_name[0:8]]\n",
    "Y = df[df_name[8]]\n",
    "X_train, X_test, y_train, y_test =train_test_split(X,Y,\n",
    "                                                   test_size=0.25,\n",
    "                                                   random_state=0,\n",
    "                                                   stratify=df['Outcome'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "SEED = 7\n",
    "np.random.seed(SEED)\n",
    "# Spot-Check Algorithms\n",
    "def GetBasedModel():\n",
    "    basedModels = []\n",
    "    basedModels.append(('LR'   , LogisticRegression()))\n",
    "    basedModels.append(('LDA'  , LinearDiscriminantAnalysis()))\n",
    "    basedModels.append(('KNN'  , KNeighborsClassifier()))\n",
    "    basedModels.append(('CART' , DecisionTreeClassifier()))\n",
    "    basedModels.append(('NB'   , GaussianNB()))\n",
    "    basedModels.append(('SVM'  , SVC(probability=True)))\n",
    "    basedModels.append(('AB'   , AdaBoostClassifier()))\n",
    "    basedModels.append(('GBM'  , GradientBoostingClassifier()))\n",
    "    basedModels.append(('RF'   , RandomForestClassifier()))\n",
    "    basedModels.append(('ET'   , ExtraTreesClassifier()))\n",
    "    return basedModels\n",
    "def BasedLine2(X_train, y_train,models):\n",
    "    # Test options and evaluation metric\n",
    "    num_folds = 10\n",
    "    scoring = 'accuracy'\n",
    "\n",
    "    results = []\n",
    "    names = []\n",
    "    for name, model in models:\n",
    "        kfold = StratifiedKFold(n_splits=num_folds, random_state=SEED,shuffle=True)\n",
    "        cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n",
    "        results.append(cv_results)\n",
    "        names.append(name)\n",
    "        msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "        print(msg)\n",
    "        \n",
    "    return names, results\n",
    "class PlotBoxR(object):\n",
    "    \n",
    "    \n",
    "    def __Trace(self,nameOfFeature,value): \n",
    "    \n",
    "        trace = go.Box(\n",
    "            y=value,\n",
    "            name = nameOfFeature,\n",
    "            marker = dict(\n",
    "                color = 'rgb(0, 128, 128)',\n",
    "            )\n",
    "        )\n",
    "        return trace\n",
    "\n",
    "    def PlotResult(self,names,results):\n",
    "        \n",
    "        data = []\n",
    "\n",
    "        for i in range(len(names)):\n",
    "            data.append(self.__Trace(names[i],results[i]))\n",
    "\n",
    "\n",
    "        py.iplot(data)\n",
    "models = GetBasedModel()\n",
    "names,results = BasedLine2(X_train, y_train,models)\n",
    "PlotBoxR().PlotResult(names,results)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "def GetScaledModel(nameOfScaler):\n",
    "    \n",
    "    if nameOfScaler == 'standard':\n",
    "        scaler = StandardScaler()\n",
    "    elif nameOfScaler =='minmax':\n",
    "        scaler = MinMaxScaler()\n",
    "\n",
    "    pipelines = []\n",
    "    pipelines.append((nameOfScaler+'LR'  , Pipeline([('Scaler', scaler),('LR'  , LogisticRegression())])))\n",
    "    pipelines.append((nameOfScaler+'LDA' , Pipeline([('Scaler', scaler),('LDA' , LinearDiscriminantAnalysis())])))\n",
    "    pipelines.append((nameOfScaler+'KNN' , Pipeline([('Scaler', scaler),('KNN' , KNeighborsClassifier())])))\n",
    "    pipelines.append((nameOfScaler+'CART', Pipeline([('Scaler', scaler),('CART', DecisionTreeClassifier())])))\n",
    "    pipelines.append((nameOfScaler+'NB'  , Pipeline([('Scaler', scaler),('NB'  , GaussianNB())])))\n",
    "    pipelines.append((nameOfScaler+'SVM' , Pipeline([('Scaler', scaler),('SVM' , SVC())])))\n",
    "    pipelines.append((nameOfScaler+'AB'  , Pipeline([('Scaler', scaler),('AB'  , AdaBoostClassifier())])  ))\n",
    "    pipelines.append((nameOfScaler+'GBM' , Pipeline([('Scaler', scaler),('GMB' , GradientBoostingClassifier())])  ))\n",
    "    pipelines.append((nameOfScaler+'RF'  , Pipeline([('Scaler', scaler),('RF'  , RandomForestClassifier())])  ))\n",
    "    pipelines.append((nameOfScaler+'ET'  , Pipeline([('Scaler', scaler),('ET'  , ExtraTreesClassifier())])  ))\n",
    "\n",
    "\n",
    "    return pipelines \n",
    "def ScoreDataFrame(names,results):\n",
    "    def floatingDecimals(f_val, dec=3):\n",
    "        prc = \"{:.\"+str(dec)+\"f}\" \n",
    "    \n",
    "        return float(prc.format(f_val))\n",
    "\n",
    "    scores = []\n",
    "    for r in results:\n",
    "        scores.append(floatingDecimals(r.mean(),4))\n",
    "\n",
    "    scoreDataFrame = pd.DataFrame({'Model':names, 'Score': scores})\n",
    "    return scoreDataFrame\n",
    "basedLineScore = ScoreDataFrame(names,results)\n",
    "models = GetScaledModel('standard')\n",
    "names,results = BasedLine2(X_train, y_train,models)\n",
    "PlotBoxR().PlotResult(names,results)\n",
    "scaledScoreStandard = ScoreDataFrame(names,results)\n",
    "compareModels = pd.concat([basedLineScore,\n",
    "                           scaledScoreStandard], axis=1)\n",
    "models = GetScaledModel('minmax')\n",
    "names,results = BasedLine2(X_train, y_train,models)\n",
    "PlotBoxR().PlotResult(names,results)\n",
    "\n",
    "scaledScoreMinMax = ScoreDataFrame(names,results)\n",
    "compareModels = pd.concat([basedLineScore,\n",
    "                           scaledScoreStandard,\n",
    "                          scaledScoreMinMax], axis=1)\n",
    "compareModels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t = df.copy()\n",
    "df_t_name = df_t.columns\n",
    "def TurkyOutliers(df_out,nameOfFeature,drop=False):\n",
    "\n",
    "    valueOfFeature = df_out[nameOfFeature]\n",
    "    # Calculate Q1 (25th percentile of the data) for the given feature\n",
    "    Q1 = np.percentile(valueOfFeature, 25.)\n",
    "\n",
    "    # Calculate Q3 (75th percentile of the data) for the given feature\n",
    "    Q3 = np.percentile(valueOfFeature, 75.)\n",
    "\n",
    "    # Use the interquartile range to calculate an outlier step (1.5 times the interquartile range)\n",
    "    step = (Q3-Q1)*1.5\n",
    "    # print \"Outlier step:\", step\n",
    "    outliers = valueOfFeature[~((valueOfFeature >= Q1 - step) & (valueOfFeature <= Q3 + step))].index.tolist()\n",
    "    feature_outliers = valueOfFeature[~((valueOfFeature >= Q1 - step) & (valueOfFeature <= Q3 + step))].values\n",
    "    # df[~((df[nameOfFeature] >= Q1 - step) & (df[nameOfFeature] <= Q3 + step))]\n",
    "\n",
    "\n",
    "    # Remove the outliers, if any were specified\n",
    "    print (\"Number of outliers (inc duplicates): {} and outliers: {}\".format(len(outliers), feature_outliers))\n",
    "    if drop:\n",
    "        good_data = df_out.drop(df_out.index[outliers]).reset_index(drop = True)\n",
    "        print (\"New dataset with removed outliers has {} samples with {} features each.\".format(*good_data.shape))\n",
    "        return good_data\n",
    "    else: \n",
    "        print (\"Nothing happens, df.shape = \",df_out.shape)\n",
    "        return df_out\n",
    "for i in range(0, len(df_t.columns)):\n",
    "    df_clean = TurkyOutliers(df_clean, df_name[i], True)\n",
    "display(df_t.shape)\n",
    "display(df_clean.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unscaled = df_clean[['Glucose','BMI','Age','DiabetesPedigreeFunction','Outcome']]\n",
    "df_imp_scaled_name = df_unscaled.columns\n",
    "df_imp_scaled = MinMaxScaler().fit_transform(df_unscaled)\n",
    "X =  df_imp_scaled[:,0:4]\n",
    "Y =  df_imp_scaled[:,4]\n",
    "X_train, X_test, y_train, y_test =train_test_split(X,Y,\n",
    "                                                   test_size=0.1,\n",
    "                                                   random_state=0,\n",
    "                                                   stratify=df_imp_scaled[:,4])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\versu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning:\n",
      "\n",
      "\n",
      "460 fits failed out of a total of 1000.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "460 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\versu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\versu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\versu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "\n",
      "C:\\Users\\versu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning:\n",
      "\n",
      "One or more of the test scores are non-finite: [       nan 0.7869026  0.78342408 0.78514822 0.7816092  0.78514822\n",
      "        nan        nan        nan        nan 0.78514822 0.78514822\n",
      "        nan 0.78339383 0.7869026  0.77108288        nan        nan\n",
      " 0.73953418        nan 0.77631579 0.78514822        nan 0.77991531\n",
      "        nan 0.78339383 0.78339383 0.78514822 0.78166969 0.78163944\n",
      " 0.78514822        nan        nan 0.78514822        nan        nan\n",
      " 0.78514822 0.78339383        nan        nan 0.77807018 0.7869026\n",
      "        nan        nan        nan 0.78339383 0.78339383 0.78514822\n",
      "        nan 0.78166969        nan        nan 0.78339383        nan\n",
      " 0.78514822        nan        nan        nan        nan        nan\n",
      " 0.77982456        nan        nan        nan        nan 0.78342408\n",
      "        nan        nan 0.78514822 0.78514822        nan        nan\n",
      " 0.7869026  0.76581972        nan        nan 0.76409558 0.78514822\n",
      " 0.78514822 0.78514822 0.70275257        nan 0.78166969 0.77991531\n",
      " 0.78339383        nan 0.78166969        nan 0.78339383 0.78514822\n",
      " 0.78514822 0.78339383        nan 0.78339383        nan        nan\n",
      "        nan 0.78339383 0.7869026  0.78514822]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.786903 using {'C': 3.730229437354635, 'penalty': 'l2'}\n",
      "Best: 0.809558 using {'n_neighbors': 15}\n",
      "prediction on test set is: 0.828125 f1 0.7027027027027027\n",
      "Best: 0.792075 using {'C': 0.1, 'kernel': 'poly'}\n",
      "prediction on test set is: 0.84375 f1 0.7058823529411764\n",
      "Best: 0.767514 using {'criterion': 'gini', 'max_depth': 3, 'max_features': 2, 'min_samples_leaf': 1}\n",
      "prediction on test set is: 0.75 f1 0.4285714285714285\n",
      "Best: 0.793829 using {'subsample': 0.6, 'n_estimators': 100, 'min_child_weight': 5, 'max_depth': 3, 'gamma': 1.5, 'colsample_bytree': 0.6}\n",
      "The test accuracy score of Gradient Boosting Classifier is  0.78125 f1 : 0.6666666666666666\n",
      "Best: 0.793860 using {'criterion': 'entropy', 'max_depth': 4, 'max_features': 'auto', 'n_estimators': 500}\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import uniform\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from scipy.stats import randint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "class GridSearch(object):\n",
    "    \n",
    "    def __init__(self,X_train,y_train,model,hyperparameters):\n",
    "        \n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.model = model\n",
    "        self.hyperparameters = hyperparameters\n",
    "        \n",
    "    def GridSearch(self):\n",
    "        # Create randomized search 10-fold cross validation and 100 iterations\n",
    "        cv = 10\n",
    "        clf = GridSearchCV(self.model,\n",
    "                                 self.hyperparameters,\n",
    "                                 cv=cv,\n",
    "                                 verbose=0,\n",
    "                                 n_jobs=-1,\n",
    "                                 )\n",
    "        # Fit randomized search\n",
    "        best_model = clf.fit(self.X_train, self.y_train)\n",
    "        message = (best_model.best_score_, best_model.best_params_)\n",
    "        print(\"Best: %f using %s\" % (message))\n",
    "\n",
    "        return best_model,best_model.best_params_\n",
    "    \n",
    "    def BestModelPridict(self,X_test):\n",
    "        \n",
    "        best_model,_ = self.GridSearch()\n",
    "        pred = best_model.predict(X_test)\n",
    "        return pred,best_model\n",
    "    \n",
    "class RandomSearch(object):\n",
    "    \n",
    "    def __init__(self,X_train,y_train,model,hyperparameters):\n",
    "        \n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.model = model\n",
    "        self.hyperparameters = hyperparameters\n",
    "        \n",
    "    def RandomSearch(self):\n",
    "        # Create randomized search 10-fold cross validation and 100 iterations\n",
    "        cv = 10\n",
    "        clf = RandomizedSearchCV(self.model,\n",
    "                                 self.hyperparameters,\n",
    "                                 random_state=1,\n",
    "                                 n_iter=100,\n",
    "                                 cv=cv,\n",
    "                                 verbose=0,\n",
    "                                 n_jobs=-1,\n",
    "                                 )\n",
    "        # Fit randomized search\n",
    "        best_model = clf.fit(self.X_train, self.y_train)\n",
    "        message = (best_model.best_score_, best_model.best_params_)\n",
    "        print(\"Best: %f using %s\" % (message))\n",
    "\n",
    "        return best_model,best_model.best_params_\n",
    "    \n",
    "    def BestModelPridict(self,X_test):\n",
    "        \n",
    "        best_model,_ = self.RandomSearch()\n",
    "        pred = best_model.predict(X_test)\n",
    "        return pred,best_model\n",
    "\n",
    "model = LogisticRegression()\n",
    "# Create regularization penalty space\n",
    "penalty = ['l1', 'l2']\n",
    "\n",
    "# Create regularization hyperparameter distribution using uniform distribution\n",
    "C = uniform(loc=0, scale=4)\n",
    "\n",
    "# Create hyperparameter options\n",
    "hyperparameters = dict(C=C, penalty=penalty)\n",
    "LR_RandSearch = RandomSearch(X_train,y_train,model,hyperparameters)\n",
    "# LR_best_model,LR_best_params = LR_RandSearch.RandomSearch()\n",
    "Prediction_LR,model_lr = LR_RandSearch.BestModelPridict(X_test)\n",
    "model_KNN = KNeighborsClassifier()\n",
    "def floatingDecimals(f_val, dec=3):\n",
    "        prc = \"{:.\"+str(dec)+\"f}\" #first cast decimal as str\n",
    "    #     print(prc) #str format output is {:.3f}\n",
    "        return float(prc.format(f_val))\n",
    "neighbors = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n",
    "param_grid = dict(n_neighbors=neighbors)\n",
    "KNN_GridSearch = GridSearch(X_train,y_train,model_KNN,param_grid)\n",
    "Prediction_KNN,knn_model = KNN_GridSearch.BestModelPridict(X_test)\n",
    "print('prediction on test set is:' ,floatingDecimals((y_test == Prediction_KNN).mean(),7),\"f1\",f1_score(y_test,Prediction_KNN))\n",
    "c_values = [0.1, 0.3, 0.5, 0.7, 0.9, 1.0, 1.3, 1.5, 1.7, 2.0]\n",
    "kernel_values = [ 'linear' , 'poly' , 'rbf' , 'sigmoid' ]\n",
    "param_grid = dict(C=c_values, kernel=kernel_values)\n",
    "model_SVC = SVC()\n",
    "SVC_GridSearch = GridSearch(X_train,y_train,model_SVC,param_grid)\n",
    "Prediction_SVC,svc_model = SVC_GridSearch.BestModelPridict(X_test)\n",
    "print('prediction on test set is:' ,floatingDecimals((y_test == Prediction_SVC).mean(),7),\"f1\",f1_score(y_test,Prediction_SVC))\n",
    "\n",
    "max_depth_value = [3, None]\n",
    "max_features_value =  randint(1, 4)\n",
    "min_samples_leaf_value = randint(1, 4)\n",
    "criterion_value = [\"gini\", \"entropy\"]\n",
    "param_grid = dict(max_depth = max_depth_value,\n",
    "                  max_features = max_features_value,\n",
    "                  min_samples_leaf = min_samples_leaf_value,\n",
    "                  criterion = criterion_value)\n",
    "model_CART = DecisionTreeClassifier()\n",
    "CART_GridSearch = RandomSearch(X_train,y_train,model_CART,param_grid)\n",
    "Prediction_CART,dt_model = CART_GridSearch.BestModelPridict(X_test)\n",
    "print('prediction on test set is:' ,floatingDecimals((y_test == Prediction_CART).mean(),7),\"f1\",f1_score(y_test,Prediction_CART))\n",
    "\n",
    "\n",
    "params = {\n",
    "        'min_child_weight': [1, 5, 10],\n",
    "        'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'max_depth': [3, 4, 5],\n",
    "        'n_estimators':[50,100,200,400]\n",
    "        }\n",
    "folds = 3\n",
    "param_comb = 5\n",
    "\n",
    "skf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)\n",
    "xgb_classifier = xgb.XGBClassifier()\n",
    "model = RandomSearch(X_train,y_train,xgb_classifier,params)\n",
    "\n",
    "\n",
    "y_pred,xgb_model = model.BestModelPridict(X_test)\n",
    "\n",
    "\n",
    "# Printing the test accuracy for Gradient Boosting Classifier\n",
    "print(\"The test accuracy score of Gradient Boosting Classifier is \",\n",
    "      accuracy_score(y_test, y_pred), \"f1 :\",  f1_score(y_test, y_pred))\n",
    "\n",
    "param_grid = { \n",
    "    'n_estimators': [200, 500],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth' : [4,5,6,7,8],\n",
    "    'criterion' :['gini', 'entropy']\n",
    "}\n",
    "random_forest= RandomForestClassifier(random_state=42)\n",
    "random_forest_gs = GridSearch(X_train,y_train,random_forest,param_grid)\n",
    "random_forest_preds = random_forest_gs.BestModelPridict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "feature_names = [0, 1, 2, 3, 4, 5]\n",
    "\n",
    "# Decision Tree\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.set_title(\"Decision Tree - Partial Dependence\")\n",
    "PartialDependenceDisplay.from_estimator(dt, X, features=feature_names, ax=ax)\n",
    "plt.show()\n",
    "\n",
    "# Logistic Regression\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.set_title(\"Logistic Regression - Partial Dependence\")\n",
    "PartialDependenceDisplay.from_estimator(\n",
    "    logreg, X_train, features=feature_names, categorical_features=[\"sex_1\"], ax=ax)\n",
    "plt.show()\n",
    "\n",
    "# SVM - Partial Dependence doesn't directly apply to SVM; consider other visualization methods\n",
    "# SVM doesn't inherently support partial dependence plots as decision tree-based models do.\n",
    "\n",
    "# XGBoost\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.set_title(\"XGBoost - Partial Dependence\")\n",
    "PartialDependenceDisplay.from_estimator(\n",
    "    xgb_classifier, X_train, features=feature_names, ax=ax, categorical_features=[\"sex_1\"])\n",
    "plt.show()\n",
    "\n",
    "# Random Forest\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.set_title(\"Random Forest - Partial Dependence\")\n",
    "PartialDependenceDisplay.from_estimator(\n",
    "    rf, X_train, features=feature_names, ax=ax, categorical_features=[\"sex_1\"])\n",
    "plt.show()\n",
    "# MULTI LAYER PERCEPTRON\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.set_title(\"MLP - Partial Dependence\")\n",
    "PartialDependenceDisplay.from_estimator(\n",
    "    mlp, X_train, features=feature_names, ax=ax, categorical_features=[\"sex_1\"])\n",
    "plt.show()\n",
    "# KNN\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.set_title(\"KNN - Partial Dependence\")\n",
    "PartialDependenceDisplay.from_estimator(\n",
    "    knn, X_train, features=feature_names, ax=ax, categorical_features=[\"sex_1\"])\n",
    "plt.show()\n",
    "# SVM\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.set_title(\"SUPPORT VECTOR MACHINE - Partial Dependence\")\n",
    "PartialDependenceDisplay.from_estimator(\n",
    "    svm, X_train, features=feature_names, ax=ax, categorical_features=[\"sex_1\"])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
