{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "import xgboost as xgb\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"./datasets/heart.csv\")\n",
    "df1 = df.copy()  # Create a copy of the dataframe\n",
    "\n",
    "# Define the columns to be encoded and scaled\n",
    "cat_cols = ['sex', 'exng', 'caa', 'cp', 'fbs', 'restecg', 'slp', 'thall']\n",
    "con_cols = [\"age\", \"trtbps\", \"chol\", \"thalachh\", \"oldpeak\"]\n",
    "\n",
    "# Encoding the categorical columns\n",
    "df1 = pd.get_dummies(df1, columns=cat_cols, drop_first=True)\n",
    "\n",
    "# Define the features and target\n",
    "X = df1.drop(['output'], axis=1)\n",
    "y = df1['output']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Normalizing the data\n",
    "scaler = MinMaxScaler()\n",
    "# X_train_normalized = scaler.fit_transform(X_train)\n",
    "# X_test_normalized = scaler.transform(X_test)\n",
    "# X_train_normalized = pd.DataFrame(X_train_normalized, columns=X.columns)\n",
    "# X_test_normalized = pd.DataFrame(X_test_normalized, columns=X.columns)\n",
    "\n",
    "X_train_normalized = X_train\n",
    "X_test_normalized = X_test\n",
    "# Instantiate and fit the SVM model\n",
    "svm = SVC(kernel='linear', C=1, random_state=42, probability=True).fit(X_train_normalized, y_train)\n",
    "\n",
    "# Predicting values\n",
    "y_pred_svm = svm.predict(X_test_normalized)\n",
    "\n",
    "# Printing the test accuracy for SVM\n",
    "print(\"The test accuracy score of SVM is \", accuracy_score(y_test, y_pred_svm), f1_score(y_test, y_pred_svm))\n",
    "\n",
    "# Instantiate and fit the Logistic Regression model\n",
    "logreg = LogisticRegression(penalty='none',max_iter=2000).fit(X_train_normalized, y_train)\n",
    "\n",
    "# Predicting values\n",
    "y_pred_logreg = logreg.predict(X_test_normalized)\n",
    "\n",
    "# Printing the test accuracy for Logistic Regression\n",
    "print(\"The test accuracy score of Logistic Regression is \", accuracy_score(y_test, y_pred_logreg), f1_score(y_test, y_pred_logreg))\n",
    "\n",
    "# Instantiate and fit the Decision Tree model\n",
    "dt = DecisionTreeClassifier(random_state=42).fit(X_train, y_train)\n",
    "\n",
    "# Predicting values\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "\n",
    "# Printing the test accuracy for Decision Tree\n",
    "print(\"The test accuracy score of Decision Tree is \", accuracy_score(y_test, y_pred_dt),\"f1 :\",  f1_score(y_test, y_pred_dt))\n",
    "\n",
    "# Instantiate and fit the Random Forest model\n",
    "rf = RandomForestClassifier().fit(X_train, y_train)\n",
    "\n",
    "# Predicting values\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "# Printing the test accuracy for Random Forest\n",
    "print(\"The test accuracy score of Random Forest is \", accuracy_score(y_test, y_pred_rf),\"f1 :\",  f1_score(y_test, y_pred_rf))\n",
    "\n",
    "# Instantiate the XGBoost classifier\n",
    "xgb_classifier = xgb.XGBClassifier(n_estimators=50, random_state=42)\n",
    "\n",
    "# Fit the XGBoost model\n",
    "xgb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict values\n",
    "y_pred_xgb = xgb_classifier.predict(X_test)\n",
    "\n",
    "# Printing the test accuracy for Gradient Boosting Classifier\n",
    "print(\"The test accuracy score of Gradient Boosting Classifier is \", accuracy_score(y_test, y_pred_xgb),\"f1 :\",  f1_score(y_test, y_pred_xgb))\n",
    "\n",
    "# Instantiate the mlp \n",
    "mlp = MLPClassifier(hidden_layer_sizes=(64, 64), max_iter=1000, random_state=42)\n",
    "\n",
    "# Fit the mlp model\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# Predict values\n",
    "y_pred_mlp = mlp.predict(X_test)\n",
    "\n",
    "# Printing the test accuracy for mlp Classifier\n",
    "print(\"The test accuracy score of MLP Classifier is \", accuracy_score(y_test, y_pred_mlp),\"f1 :\",  f1_score(y_test, y_pred_mlp))\n",
    "\n",
    "# Instantiate the KNN \n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# Fit the mlp model\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict values\n",
    "y_pred_knn = knn.predict(X_test)\n",
    "\n",
    "# Printing the test accuracy for mlp Classifier\n",
    "print(\"The test accuracy score of knn Classifier is \", accuracy_score(y_test, y_pred_mlp),\"f1 :\",  f1_score(y_test, y_pred_knn))\n",
    "# Calculate permutation importance for SVM\n",
    "perm_importance_svm = permutation_importance(svm, X_test_normalized, y_test, n_repeats=30, random_state=42,n_jobs=50)\n",
    "\n",
    "# Get feature importances\n",
    "rf_importances = rf.feature_importances_\n",
    "xgb_importances = xgb_classifier.feature_importances_\n",
    "logreg_importances = abs(logreg.coef_[0])\n",
    "current_importance_type = xgb_classifier.get_booster().get_score()\n",
    "print(current_importance_type)\n",
    "# Create a DataFrame with feature importances\n",
    "feature_importances = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'RandomForest': rf_importances,\n",
    "    'DecissionTree': dt.feature_importances_,\n",
    "    'XGBoost': xgb_importances,\n",
    "    'LogisticRegression': logreg_importances,\n",
    "    'SVM': perm_importance_svm.importances_mean,\n",
    "})\n",
    "\n",
    "# Display feature importances\n",
    "display(feature_importances)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "feature_names = [0, 1,2, 3,4,5]\n",
    "\n",
    "# Decision Tree\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.set_title(\"Decision Tree - Partial Dependence\")\n",
    "PartialDependenceDisplay.from_estimator(dt, X, features=feature_names, ax=ax)\n",
    "plt.show()\n",
    "\n",
    "# Logistic Regression\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.set_title(\"Logistic Regression - Partial Dependence\")\n",
    "PartialDependenceDisplay.from_estimator(logreg, X_train, features=feature_names,categorical_features= [\"sex_1\"], ax=ax)\n",
    "plt.show()\n",
    "\n",
    "# SVM - Partial Dependence doesn't directly apply to SVM; consider other visualization methods\n",
    "# SVM doesn't inherently support partial dependence plots as decision tree-based models do.\n",
    "\n",
    "# XGBoost\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.set_title(\"XGBoost - Partial Dependence\")\n",
    "PartialDependenceDisplay.from_estimator(xgb_classifier, X_train, features=feature_names, ax=ax,categorical_features= [\"sex_1\"])\n",
    "plt.show()\n",
    "\n",
    "# Random Forest\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.set_title(\"Random Forest - Partial Dependence\")\n",
    "PartialDependenceDisplay.from_estimator(rf, X_train, features=feature_names, ax=ax,categorical_features= [\"sex_1\"])\n",
    "plt.show()\n",
    "# MULTI LAYER PERCEPTRON\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.set_title(\"MLP - Partial Dependence\")\n",
    "PartialDependenceDisplay.from_estimator(mlp, X_train, features=feature_names, ax=ax,categorical_features= [\"sex_1\"])\n",
    "plt.show()\n",
    "# KNN\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.set_title(\"KNN - Partial Dependence\")\n",
    "PartialDependenceDisplay.from_estimator(knn, X_train, features=feature_names, ax=ax,categorical_features= [\"sex_1\"])\n",
    "plt.show()\n",
    "# SVM\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.set_title(\"SUPPORT VECTOR MACHINE - Partial Dependence\")\n",
    "PartialDependenceDisplay.from_estimator(svm, X_train, features=feature_names, ax=ax,categorical_features= [\"sex_1\"])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data was extracted from the 1994 Census bureau database by Ronny Kohavi and Barry Becker (Data Mining and Visualization, Silicon Graphics). A set of reasonably clean records was extracted using the following conditions: ((AAGE>16) && (AGI>100) && (AFNLWGT>1) && (HRSWK>0)). The prediction task is to determine whether a person makes over $50K a year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pdpbox, lime, shap\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "data = pd.read_csv(\"./datasets/adult.csv\")\n",
    "data.shape\n",
    "data['target']=data['income'].map({'<=50K':0,'>50K':1})\n",
    "data.drop(\"income\",axis=1,inplace=True)\n",
    "data['target'].value_counts()\n",
    "data.drop(\"education.num\",axis=1,inplace=True)\n",
    "data.drop('native.country',axis=1,inplace=True)\n",
    "data=pd.get_dummies(data, drop_first = True)\n",
    "y = data['target'].values\n",
    "features = [col for col in data.columns if col not in ['target']]\n",
    "X = data[features]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, test_size=0.3, stratify=y)\n",
    "\n",
    "# Normalizing the data\n",
    "scaler = MinMaxScaler()\n",
    "# X_train_normalized = scaler.fit_transform(X_train)\n",
    "# X_test_normalized = scaler.transform(X_test)\n",
    "# X_train_normalized = pd.DataFrame(X_train_normalized, columns=X.columns)\n",
    "# X_test_normalized = pd.DataFrame(X_test_normalized, columns=X.columns)\n",
    "\n",
    "X_train_normalized = scaler.fit_transform(X_train)\n",
    "X_test_normalized = scaler.fit_transform(X_test)\n",
    "# Instantiate and fit the SVM model\n",
    "svm = SVC(kernel='linear', C=1, random_state=42, probability=True).fit(X_train_normalized, y_train)\n",
    "\n",
    "# Predicting values\n",
    "y_pred_svm = svm.predict(X_test_normalized)\n",
    "\n",
    "# Printing the test accuracy for SVM\n",
    "print(\"The test accuracy score of SVM is \", accuracy_score(y_test, y_pred_svm),\"f1 :\",  f1_score(y_test, y_pred_svm))\n",
    "\n",
    "# Instantiate and fit the Logistic Regression model\n",
    "logreg = LogisticRegression(penalty='none',max_iter=2000).fit(X_train_normalized, y_train)\n",
    "\n",
    "# Predicting values\n",
    "y_pred_logreg = logreg.predict(X_test_normalized)\n",
    "\n",
    "# Printing the test accuracy for Logistic Regression\n",
    "print(\"The test accuracy score of Logistic Regression is \", accuracy_score(y_test, y_pred_logreg),\"f1 :\", f1_score(y_test, y_pred_logreg))\n",
    "\n",
    "# Instantiate and fit the Decision Tree model\n",
    "dt = DecisionTreeClassifier(random_state=42).fit(X_train, y_train)\n",
    "\n",
    "# Predicting values\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "\n",
    "# Printing the test accuracy for Decision Tree\n",
    "print(\"The test accuracy score of Decision Tree is \", accuracy_score(y_test, y_pred_dt),\"f1 :\",  f1_score(y_test, y_pred_dt))\n",
    "\n",
    "# Instantiate and fit the Random Forest model\n",
    "rf = RandomForestClassifier().fit(X_train, y_train)\n",
    "\n",
    "# Predicting values\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "# Printing the test accuracy for Random Forest\n",
    "print(\"The test accuracy score of Random Forest is \", accuracy_score(y_test, y_pred_rf),\"f1 :\",  f1_score(y_test, y_pred_rf))\n",
    "\n",
    "# Instantiate the XGBoost classifier\n",
    "xgb_classifier = xgb.XGBClassifier(n_estimators=50, random_state=42)\n",
    "\n",
    "# Fit the XGBoost model\n",
    "xgb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict values\n",
    "y_pred_xgb = xgb_classifier.predict(X_test)\n",
    "\n",
    "# Printing the test accuracy for Gradient Boosting Classifier\n",
    "print(\"The test accuracy score of Gradient Boosting Classifier is \", accuracy_score(y_test, y_pred_xgb),\"f1 :\",  f1_score(y_test, y_pred_xgb))\n",
    "\n",
    "# Instantiate the mlp \n",
    "mlp = MLPClassifier(hidden_layer_sizes=(64, 64), max_iter=1000, random_state=42)\n",
    "\n",
    "# Fit the mlp model\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# Predict values\n",
    "y_pred_mlp = mlp.predict(X_test)\n",
    "\n",
    "# Printing the test accuracy for mlp Classifier\n",
    "print(\"The test accuracy score of MLP Classifier is \", accuracy_score(y_test, y_pred_mlp),\"f1 :\",  f1_score(y_test, y_pred_mlp))\n",
    "\n",
    "# Instantiate the KNN \n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# Fit the mlp model\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict values\n",
    "y_pred_knn = knn.predict(X_test)\n",
    "\n",
    "# Printing the test accuracy for mlp Classifier\n",
    "print(\"The test accuracy score of knn Classifier is \", accuracy_score(y_test, y_pred_mlp), f1_score(y_test, y_pred_knn))\n",
    "# Calculate permutation importance for SVM\n",
    "perm_importance_svm = permutation_importance(svm, X_test_normalized, y_test, n_repeats=20, random_state=42,n_jobs=20)\n",
    "perm_importance_knn = permutation_importance(knn, X_test_normalized, y_test, n_repeats=20, random_state=42,n_jobs=20)\n",
    "\n",
    "\n",
    "# Get feature importances\n",
    "rf_importances = rf.feature_importances_\n",
    "xgb_importances = xgb_classifier.feature_importances_\n",
    "logreg_importances = abs(logreg.coef_[0])\n",
    "current_importance_type = xgb_classifier.get_booster().get_score()\n",
    "print(current_importance_type)\n",
    "# Create a DataFrame with feature importances\n",
    "feature_importances = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'RandomForest': rf_importances,\n",
    "    'DecissionTree': dt.feature_importances_,\n",
    "    'XGBoost': xgb_importances,\n",
    "    'LogisticRegression': logreg_importances,\n",
    "    'KNN': perm_importance_knn,\n",
    "    'SVM': perm_importance_svm.importances_mean,\n",
    "})\n",
    "\n",
    "# Display feature importances\n",
    "display(feature_importances)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
